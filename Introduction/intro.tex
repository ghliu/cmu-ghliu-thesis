\documentclass[../thesis.tex]{subfiles}

\begin{document}


%%%  Introduce for Autonomous Driving : Motivation %%%

The rapid urbanization globally in the recent past has led to severe road congestion, rise in pollution levels and an increase in road accidents, therefore presenting a very grim picture of the current state of urban transportation. At the moment, private automobiles are widely recognized as an unsustainable solution for the future of personal urban mobility \cite{reinventing}. Fortunately however, great strides have been made in the development of autonomous driving technologies, energized by the successful demonstrations by some teams at the DARPA Urban Challenge \cite{boss, multimodaltartan}. While offering an opportunity to develop sustainable and safe solutions to personal mobility \cite{usecases_of_AD}, they also hint at a complete overhaul of the urban transportation landscape by ushering in Autonomous Vehicles-on-Demand. 

%%% Off-road navigation %%%

Although planning in urban environment is a well-studied research field [cite cmu/standard urban challenge paper], and can be efficiently solved with graph-based planner [cite D*] followed by the trajectory optimization techniques [cite standard practical XX]. 
Motion planning for unmanned ground vehicle on rough terrain has still been an on-going research topics. Built upon the successful of the XXX \cite{}, research 

such as aggressive behavior 
such as drifting \cite{}.

One of the main challenges comes from the complexity of the off-road field environment. While a simple kinematic model may be enough to tackle urban environment, maneuvering agile requires a much complex vehicle dynamic model with potentially a higher dimensional state space representation. 

and the standard graph-based approach breaks down. Recently, there has been an active focuses on aggressive motion planning with off-road navigation.  


can be treated as a well-investigated kinodynamic planning problem in control space, several challenges raised when operating in off-road environment. 
First, the vehicle dynamic in off-road environment is much more unpredictable in contrast to on-road condition. Factor such as wheel-terrain interaction for modeling the sliding effect is still an active research area \todo{cite some papers}. 
Secondly, developing a local planner for high speed operation can be categorized as an anytime planning problem. Thus, computational efficiency should be taken into account for the real-time concern.

%% Stochastic Regularization

Stochastic regularization is an active area of research in deep learning made popular by the success of, \textit{Dropout} \cite{dropout}. Following this landmark paper, numerous extensions were proposed  to further generalize this idea such as \textit{Blockout} \cite{blockout}, \textit{DropConnect} \cite{dropconnect}, \textit{Zoneout} \cite{zoneout}, etc. In the similar vein, two interesting techniques have been proposed for specialized regularization in the multi-modal setting namely ModDrop \cite{moddrop} and ModOut \cite{modout}. 
Given a much wider set of sensors to choose from, ModOut attempts to identify which sensors are actually needed to fully observe the system behavior. This is out of the scope of this work. Here, we assume that all the sensors are critical and we only focus on improving the state information based on inputs from multiple observers. 
ModDrop is much closer in spirit to the proposed \emph{Sensor Dropout (SD)}. However, unlike ModDrop, pre-training with individual sensor inputs using separate loss functions is not required. A network can be directly constructed in an end-to-end fashion and \emph{Sensor Dropout} can be directly applied at the sensor fusion layer just like Dropout. Its appeal lies in its simplicity during implementation and is designed to be applicable even to the DRL setting. As far as we know, this is the first attempt at applying stochastic regularization in a DRL setting with the spirit of sensor fusion. 


% \section{Approach}

% \section{Planning}


% \section{The Rise and Challenge of End-to-end Learning}

%%% Deep end-to-end learning %%%
In the past few years, there is a great interest in applying a more end-to-end approach \cite{deepdriving,nvidiacar,endtoendcars} wherein one can learn a complex mapping that goes directly from the input to the output by leveraging the availability to a large volume of task specific data. More recently, the approach has been pushed one step forward thanks to the success of deep reinforcement learning (DRL), which has shown to achieve human-level performance on many gaming environments \cite{mnih2013playing, mnih2015human,2016-TOG-deepRL}. DRL provides a much better formulation that allows policy improvement with feedback, while the traditional deep supervised learning-based driving requires labeling and may not be able to deal with the problem of accumulating errors \cite{ross2011reduction}. 

%%% Multi modal end-to-end %%%
Despite the successful applications of deep neural net as a highly non-linear function approximator, it is often criticized as very data-sensitive and performing more like a purely black box. These characteristics prevent the engineers from semantically investigating the some crucial property such as robustness and generalization in order to extend 
to the real-world applications.


thus not intuitively allow engineers to semantic 
. For the 

In fact, with a closer look at the recent rise of autonomous driving, 

sensitive to training data and often used as a black box 

one of the key challenges for this approach to extend to the end-to-end controller in robotics applications is the black-box nature of the deep neural net.

the lack of the capability to visualize the semantic meaning. 

Despite the powerful deep neural net as a function approximator introduced by, one of the key challenges for 
However, one of the key challenges for the end-to-end approach is to 
hard to visualize semantic meaning 
black box -> no guarantee on 
raise safety concern ... 
no use of sensor fusion

It is clear sensor fusion is indispensable for autonomous driving, in order to improve accuracy and robustness in the vehicle's algorithmic decision making. Indeed, multi-modal perception was an integral part of autonomous navigation solutions and even played a critical role in their success \cite{multimodaltartan} before the advent of end-to-end deep learning based approaches. Sensor fusion offers several advantages namely robustness to individual sensor noise/failure, improved object classification and tracking \cite{elfring2016multisensor, cho2014multi, darms2008classification}, robustness to varying weather and environmental conditions, etc. 

Multi-modal deep learning, in general, is an active area of research in other domains like audiovisual systems \cite{ngmultimodal}, gesture recognition \cite{moddrop}, text/speech and language models \cite{languagemultimodal,srivastava2012multimodal}, etc. However, in the space of end-to-end sensorimotor control for autonomous navigation, this multi-modal outlook has not received much attention and is need of the hour.

% Multi modal DL related works
Currently, the most promising demonstration of an effective end-to-end framework for autonomous driving is from NVIDIA \cite{nvidiacar}. While this work has some form of sensor fusion, it should be noted that they fuse information from the \textit{same} type of sensor (cameras in this case) but placed in different, but strategic, locations on the car. We argue that a more robust strategy for sensor-fusion is to obtain information from different type of sensors. 

% Multi modal DRL related works
Recently, there is also a great interest on extending DRL approaches to multi-input fashion in order to tackle complex robotics tasks such as human-robot-interaction \cite{qureshi2016robot} and manipulation \cite{levine2016end}. Recently, \citet{mirowski2017a} proposed an novel approach namely \textit{NAV A3C} and uses information such as vision, depth, and agent velocity for maze navigation. However, information such as depth is only used during training as an auxiliary loss, and it is absent during testing. Moreover, the system dynamic in maze is relatively simple compared with autonomous driving. Here, we are more interested 
in the aspect of sensor fusion
No partial sensor failure is addressed.


We argue that this problem is critical as a further step toward the real-world robotics application given the current state-of-the-art DRL agents on many realistic simulators. 

%%%%%%%%%%%%%%%%%%%


\end{document}
