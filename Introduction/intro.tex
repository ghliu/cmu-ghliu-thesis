\documentclass[../thesis.tex]{subfiles}

% TODO
% find a place to add DIRL intro

\begin{document}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[height=3.5cm]{./Introduction/fig/urban_challenge.jpg}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[height=3.5cm]{./Introduction/fig/uber.jpg}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[height=3.5cm]{./Introduction/fig/crusher.jpg}
    \end{subfigure}
    \caption{Some of the well-known autonomous navigation systems: (a) The Carnegie Mellon Tartan that won the Urban Challenge in 2007 \cite{boss}. (b) Uber's self-driving vehicle. (c) Crusher. \cite{stentz2007crusher}}
    \label{fig:uber_rock}
\end{figure}

\section{Motivation}

%%%  Introduce for on/off-road Autonomous Driving : Motivation %%%

The rapid urbanization globally in the recent past has led to severe road congestion, rise in pollution levels and an increase in road accidents, therefore presenting a very grim picture of the current state of urban transportation. 
At the moment, private automobiles are widely recognized as an unsustainable solution for the future of personal urban mobility \cite{reinventing}. 
Fortunately however, great strides have been made in the development of autonomous driving technologies, energized by the successful demonstrations by some teams at the DARPA Urban Challenge \cite{boss, multimodaltartan}. 
By offering an opportunity to develop sustainable and safe solutions to personal mobility \cite{usecases_of_AD}, they also hint at a complete overhaul of the urban transportation landscape by ushering in Autonomous Vehicles-on-Demand. 
While there is a foreseeable trend on operating autonomous driving technologies on-road within decade, extreme/aggressive motion planning in the off-road situations or unstructured terrains has gained great interests recently \cite{kolter2010probabilistic,williams2016aggressive,gray2012predictive,cutler2016autonomous,cutler2014reinforcement}. 

\subsection{Traditional Motion Planner}

%%% Intro of Graph base and sample base planner %%%

The two most common approaches for ground vehicles motion planning are graph based planner and sample based planner. 
Both approaches have shown their robustnesses in the DRAPA Urban Challenge \cite{koenig2002d,kuwata2008motion} and UPI program \cite{kelly2006toward,stentz2007crusher}. The feasible trajectory can be solved efficiently under certain reasonable assumptions, then fine-tuned with optimization techniques to ensure criteria such as smoothness and kinematic constraint \cite{dolgov2008practical}.

%%% Pros and cons of two approach applied to off-road (extreme) navigation %%%

Though graph based planner has shown a great development on off-road navigation system \cite{kelly2006toward,stentz2007crusher}, it is limited to the low dimensional configuration space.
In fact, one of the main challenges for off-road motion planning comes from the fact that the vehicle dynamics in off-road environment is much more unpredictable in contrast to the on-road conditions. 
Factors such as wheel-terrain interaction for modeling the sliding effect is still an active research area \cite{shibly2005equivalent,rubinstein2004detailed}. 
Maneuvering agilely and aggressively on the rough terrain requires a much complex vehicle model, which inevitably leads to a higher dimensional configuration space, and thus slows down the standard graph based approaches. 

Sample based planner on the other hand is more efficient in solving higher-dimensional motion planning. 
Built upon the successful applications on many robotics problems such as manipulations \cite{kuffner2000rrt}, researchers have tried to extend the approach to an aggressive motion planner such as drifting \cite{hwan2011anytime}. 
However, unlike graph based planner that guarantees a certain level of optimality, sample based planner in general only permits asymptotic convergence to the optimal solution. Moreover, it is not straight-forward to implement in kinodynamic problems.

%%% Summary of the traditional motion planner (Link to end-to-end in the next section) %%%

Despite the pros and cons of each approach, they both share a common frameworks where a intelligent pipeline is built to efficiently gather incoming sensor data and take suitable control actions with good repeatability and fault-tolerance. 
The resulting autonomous navigation system is addressed in a modular fashion, where specialized algorithms were developed for each sub-system and finally integrated with some fine tuning.


\subsection{Learning End-to-end Controller}

\begin{figure}[b]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[height=4cm]{./Introduction/fig/gta.jpg}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[height=4cm]{./Introduction/fig/nvidia.jpg}
    \end{subfigure}
    \caption{Demonstrations of the end-to-end approach on autonomous navigation applied to (a) the realistic simulator (GTA V), and (b) the full-size vehicle by NVIDIA\cite{nvidiacar}.}
    \label{fig:end-to-end}
\end{figure}

%%% Intro of end-to-end learning  %%%

More recently, however, there is a great interest in applying an end-to-end approach wherein one can learn a complex mapping that goes directly from the input to the output by leveraging the availability to a large volume of task specific data. 
This approach is purported to perform better since we are optimizing the whole system. 
The end-to-end approaches have become more appealing with the use of deep learning in robotics and has shown successes in developing visuomotor policies for autonomous driving \cite{deepdriving,nvidiacar,endtoendcars}. 
However, the traditional deep supervised learning-based driving requires a great deal of human annotation and may not be able to deal with the problem of accumulating errors \cite{ross2011reduction}. 

%%%  Our choice of using DRL instead of RL  %%%

On the other hand, deep reinforcement learning (DRL) offers a better formulation that allows policy improvement with feedback, and has been shown to achieve human-level performance on several video games \cite{mnih2013playing, mnih2015human,2016-TOG-deepRL}.
The choice of DRL over the more tried and tested waters of Supervised Learning comes from the interest of better generalization and allowing for a more exploration-friendly setting. 
Recently, there have been a great demands and improvements on realistic simulator \cite{deepdrive,udacity} with the hope that the acquired learning in such a simulator can be transferable to the real world with minimal tuning \cite{you2017virtual}. 
% We are inspired by this new development that provides the potential to work around the large sampling requirements of DRL algorithms, and we wish to extend its use to autonomous driving.

%%% Multimodal DRL %%%

Previous work in DRL predominantly learned policies based on a single input modality, i.e., either low-dimensional physical states, or high-dimensional pixels. 
For autonomous driving where enhancing safety and accuracy to the maximum possible extent is top priority, developing policies that operate with multiple inputs is the need of the hour. 
In this light, several resent works in DRL have tried to solve the complex robotics tasks such as human-robot-interaction \cite{qureshi2016robot}, manipulation \cite{levine2016end} and maze navigation \cite{mirowski2017a} with multimodal inputs. 
However, none of the works have explicitly formulated the multimodal sensor policy in term of sensor fusion, a indispensable technique that improve accuracy and robustness in an autonomous navigation setting. 
This problem is critical as a further step toward the real-world robotics application given the current state-of-the-art DRL agents on many realistic simulators.

In fact, multimodal perception was an integral part of autonomous navigation solutions and even played a critical role in their success \cite{multimodaltartan} before the advent of end-to-end deep learning based approaches. 
It offers several advantages, namely robustness to individual sensor noise/failure, improved object classification and tracking \cite{elfring2016multisensor, cho2014multi, darms2008classification}, robustness to varying weather and environmental conditions, etc. 
It is worth mentioning that though \citet{mirowski2017a} uses multimodal sensors to navigate through a maze, information such as depth is only used as an auxiliary loss, i.e. it is not an input to the trained policy but is only used to improve the learning outcomes. 
In fact, \citet{mirowski2017a} pointed out that the naive RGBD policy performs worse than predicting depth as a regression task,


\begin{figure}[b]
	\vskip -0.1in
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[height=3.5cm]{./Introduction/fig/drl_manipulation.jpg}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[height=3.5cm]{./Introduction/fig/drl_perfer_network.jpg}
    \end{subfigure}
    \caption{Recent successes on applying deep reinforcement learning (DRL) algorithms to real-world robotics problems: (a) manipulation \cite{gu2016deep}. (b) autonomous navigation. \cite{prefernetwork}}
    \label{fig:end-to-end}
\end{figure}


\section{Approaches}

In this work, we first proposed a model-based local planner solve the optimal kinodynamic motion planning problem. 
The local planner is built upon a vehicle model that effectively captures important state for off-road navigation such as lateral velocity. 
Practical problems such as re-planning and optimization are addressed under the standard sample based approach, but modified appropriately to encourage a more aggressive maneuvering. 
The resulting model-predictive planner was tested on the full-size All-Terrain Vehicle (ATV) in the off-road conditions.

In addition, we also present an alternative end-to-end controller that uses multi-sensor input to learn an autonomous navigation policy in a physics-based gaming environment called TORCS \cite{wymann2000torcs}. 
To show the effectiveness of multi-modal perception, we picked two popular continuous action DRL algorithms namely Normalized Advantage Function (NAF) \cite{CDQN} and Deep Deterministic Policy Gradient (DDPG) \cite{DBLP:journals/corr/LillicrapHPHETS15}, and augmented them to accept multimodal inputs. 
Though a multimodal sensor policy may greatly improve the reward, it might rely heavily on all the inputs to the extent that it may fail completely even if a single sensor broke down fully or partially. This undesirable consequence renders sensor redundancy useless. 
To ensure that the learned policy does not succumb to such over-fitting, we apply a novel stochastic regularization method called \emph{Sensor Dropout} during training. 
Our approach reduces the policy sensitivity to a particular sensor subset, and make it capable of functioning even in the face of partial sensor failure. 
Based on the Sensor Dropout, we further embedded the standard DRL loss with an auxiliary loss that help reduce the action variations of the multi-modal policy. 
As far as we know, we are the first to address the multi-modal policy learning in terms of sensor fusion.

Through empirical testing we demonstrate that our proposed multimodal sensor policy can 1) operate with minimal performance drop in noisy environments, and 2) remain functional even in the face of a sensor subset failure. 
Finally, through the visualization of gradients, we show that the learned policies are conditioned on the same latent input distribution despite having multiple sensory observations spaces - a hallmark of true sensor-fusion.
Suitable metrics have been devised to study this behavior and highlight its applicability to other domains.

Note that either the traditional or the end-to-end planner require the cost/reward heuristic function to guild the policy and formally define the optimization problem. 
However, unlike in the urban environment where cost function can be well defined in a rule-based structure, finding a cost function for off-road navigation can be nontrivial and often requires lot of handy tuning. \cite{silver2010learning}
Here, motivated by the recent successes \cite{wulfmeier2015maximum,wulfmeier2016watch}, we also investigate into the using deep neural network as the function approximator, and infer the cost/reward function under inverse reinforcement learning platform.

The thesis is organized as follow:
In Chapter \ref{chap:related_work} we briefly go through the preliminary background, including the basic sample based planner, the formal definition of deep (inverse) reinforcement learning, and the two continuous action agents used in this work.
In Chapter \ref{chap:rrtplanner} we formulate our model predictive planner, followed with the derivation of vehicle model, detailed planner design for optimal planning, and finally the experimental results on field.
In Chapter \ref{chap:multimodalDRL} we propose a new technique at the aspect of sensor fusion in order to improve the performance of the end-to-end controller with multi-sensor inputs. 
In Chapter \ref{chap:dirl} we investigate the traversibility analysis with the recently popular studies on deep inverse reinforcement learning.
Finally, we conclude the work in \ref{chap:conclusion} and address some of the interesting direction for future works.

\end{document}
