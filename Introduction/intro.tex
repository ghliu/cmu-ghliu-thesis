\documentclass[../thesis.tex]{subfiles}

\begin{document}

%%%  Introduce for Autonomous Driving : Motivation %%%
% The rapid urbanization globally in the recent past has led to severe road congestion, rise in pollution levels and an increase in road accidents, therefore presenting a very grim picture of the current state of urban transportation. At the moment, private automobiles are widely recognized as an unsustainable solution for the future of personal urban mobility \cite{reinventing}. Fortunately however, great strides have been made in the development of autonomous driving technologies, energized by the successful demonstrations by some teams at the DARPA Urban Challenge \cite{boss, multimodaltartan}. While offering an opportunity to develop sustainable and safe solutions to personal mobility \cite{usecases_of_AD}, they also hint at a complete overhaul of the urban transportation landscape by ushering in Autonomous Vehicles-on-Demand. 

%%%  Introduce for Autonomous Driving : Alternative learning-based approach %%%
One of the key challenges to building accurate and robust autonomous navigation systems is to develop a strong intelligence pipeline that is able to efficiently gather incoming sensor data and take suitable control actions with good repeatability and fault-tolerance. In the past, this was addressed in a modular fashion, where  specialized algorithms were developed for each sub-system and later integrated with some fine tuning. More recently however, there is great interest in applying a more end-to-end approach wherein one can learn a complex mapping that goes directly from the input to the output by leveraging the availability to a large volume of task specific data. This approach is purported to perform better since we are optimizing the whole system. 
% \todo{mention why this is better}

%%% Introduce Deep Learning and Deep Reinforcement Learning : Motivation on true AI for decision making %%%%%%%%

This end-to-end approach has now become a viable alternative thanks to the extension of deep learning's success to robotics and has been applied successfully to autonomous driving \cite{deepdriving,nvidiacar,endtoendcars}. 
% in diverse domains like speech recognition\cite{graves2013speech,hinton2012deep}\todo{someone remove these?}, computer vision\cite{krizhevsky2012imagenet,simonyan2014very}, and 
However, the traditional deep supervised learning-based driving requires labeling and may not be able to deal with the problem of accumulating errors \cite{ross2011reduction}. On the other hand, deep reinforcement learning (DRL) provides a much better formulation that allows policy improvement with feedback, and has been shown to achieve human-level performance on many gaming environments \cite{mnih2013playing, mnih2015human,2016-TOG-deepRL}.

%%% Motivation of this work: Multi-modal Perception, Multi-modal Deep learning %%%%%%%%
Previous work in DRL predominantly learned policies based on a single input modality, i.e., either low-dimensional physical states, or high-dimensional pixels. For autonomous driving task where enhancing safety and accuracy to the maximum extent possible is emphasized, developing policies that operate with multiple inputs is crucial. Indeed, multi-modal perception was an integral part of autonomous navigation solutions and even played a critical role in their success \cite{multimodaltartan} before the advent of end-to-end deep learning based approaches. Sensor fusion offers several advantages namely robustness to individual sensor noise/failure, improved object classification and tracking \cite{elfring2016multisensor, cho2014multi, darms2008classification}, robustness to varying weather and environmental conditions, etc. 
% Previous work in DRL predominantly learned policies based on a single input modality, i.e., either low-dimensional physical states, or high-dimensional pixels. We have to however keep in mind the importance of enhancing safety and accuracy to the maximum extent possible in autonomous driving. In this light, we believe that it is worth exploring tangible ways to develop policies that operate with multiple inputs. 

Recently, there is also an great progress on extending DRL approaches to multi-input fashion in order to tackle complex robotics tasks such as human-robot-interaction \cite{qureshi2016robot} and manipulation \cite{levine2016end}. Recently, \citet{mirowski2017a} proposed an novel approach namely \textit{NAV A3C} and embedded information such as vision, depth, and agent velocity for maze navigation. However, it is worth mentioned that the system dynamic is relatively simple compared with autonomous driving.
% Multi-modal perception was an integral part of autonomous navigation solutions and even played a critical role in their success \cite{multimodaltartan} before the advent of end-to-end deep learning based approaches. Sensor fusion offers several advantages namely robustness to individual sensor noise/failure, improved object classification and tracking \cite{elfring2016multisensor, cho2014multi, darms2008classification}, robustness to varying weather and environmental conditions, etc. Multi-modal deep learning, in general, is an active area of research in other domains like audiovisual systems \cite{ngmultimodal}, gesture recognition \cite{moddrop}, text/speech and language models \cite{languagemultimodal,srivastava2012multimodal}, etc. However, Multi-modal learning is conspicuous by its absence in the modern end-to-end autonomous navigation literature. 

%%% Brief summary of the contribution of this work  %%%%%%%%
In this work, we present an end-to-end controller that uses multi-sensor input to learn an autonomous navigation policy in a physics-based gaming environment called TORCS \cite{wymann2000torcs}. To show the effectiveness of Multi-modal Perception, we picked two popular continuous action DRL algorithms namely Normalized Advantage Function (NAF) \cite{CDQN} and Deep Deterministic Policy Gradient (DDPG) \cite{DBLP:journals/corr/LillicrapHPHETS15}, and augmented them to accept multi-modal input. We limit our objective to only achieving autonomous navigation without any obstacles or other cars. This problem is kept simpler deliberately to focus our attention more on analyzing the performance of the proposed multi-modal configurations using extensive quantitative and qualitative testing. We believe however that it is representative of the larger urban navigation problem, and we ensure that our proposed network architecture is task-agnostic and transferable. We show through extensive empirical testing that a multi-modal deep reinforcement learning architecture achieves higher average reward.
% \todo{add more details about results}

Moreover, we also observe that while a multi-modal policy greatly improves the reward, it might rely heavily on all the inputs to the extent that it may fail completely even if a single sensor broke down fully or partially. This undesirable consequence renders sensor redundancy useless. To ensure that the learned policy does not succumb to such over-fitting, we apply a novel stochastic regularization method called \emph{Sensor Dropout} during training. 

%%% Intro : Summarize different current approaches Dropout, DropConnect, ModDrop, Zoneout, Blockout, Modout, etc. %%%
Stochastic regularization is an active area of research in deep learning made popular by the succes of, \textit{Dropout} \cite{dropout}. Following this landmark paper, numerous extensions were proposed  to further generalize this idea such as \textit{Blockout} \cite{blockout}, \textit{DropConnect} \cite{dropconnect}, \textit{Zoneout} \cite{zoneout}, etc. In the similar vein, two interesting techniques have been proposed for specialized regularization in the multi-modal setting namely ModDrop \cite{moddrop} and ModOut \cite{modout}. Given a much wider set of sensors to choose from, ModOut attempts to identify which sensors are actually needed to fully observe the system behavior. This is out of the scope of this work. Here, we assume that all the sensors are critical and we only focus on improving the state information based on inputs from multiple observers. ModDrop is much closer in spirit to the proposed \emph{Sensor Dropout (SD)}. However, unlike ModDrop, pretraining with individual sensor inputs using separate loss functions is not required. A network can be directly constructed in an end-to-end fashion and \emph{Sensor Dropout} can be directly applied at the sensor fusion layer just like Dropout. Its appeal lies in its simplicity during implementation and is designed to be applicable even to the DRL setting. As far as we know, this is the first attempt at applying stochastic regularization in a DRL setting. We show extensive simulation results to validate the net improvement in performance and robustness with Sensor Dropout. 

Through extensive empirical testing we show the following exciting results in this paper:
\begin{enumerate}
\item Multimodal-DRL with Sensor Dropout (SD) reduces performance drop in a noisy environment from $\approx 30\%$ to just $5\%$, when compared to a baseline single sensor system.
\item Policies learned using SD best leverage the multi-modal setting by greatly reducing over-dependence on any one sensing modality. Additionally, for each sensor it was observed that SD enforces sparsity and promotes each sensor to base the policy primarily on intuitive and salient features.
\item A multi-modal policy with SD guarantees functionality even in a face a sensor failure. This is a huge plus and the best use-case for the need for redundancy in safety-critical application like autonomous navigation.
\end{enumerate}
% \todo{come back and carefully review this last para after the finishing the paper. Highlight some key results here to grab attention} 
%%% Paper Summary %%%

% The report is organized as follows. Section 2 summarizes the background and two DRL algorithms we extended in this work. Section 3 introduces Multi-modal network architecture, and propose a new stochastic regularization technique called \emph{Sensor Dropout}. The performance of Sensor Dropout is then validated in Section 4, with the problem setup in the TORCS simulator, and other testing results. In Section 5, we summarize our results and discuss key insights obtained through this exercise. Finally, Section 6 contains conclusions and ideas for future work. 

\end{document}
